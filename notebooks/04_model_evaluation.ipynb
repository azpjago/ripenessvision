{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c186c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Model Evaluation - ripenessVision\n",
    "# \n",
    "# Evaluasi mendalam model yang sudah ditraining\n",
    "\n",
    "# %%\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b536a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Setup\n",
    "BASE_DIR = Path('..')\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "PROCESSED_DIR = BASE_DIR / 'data' / 'processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad7dc7",
   "metadata": {},
   "source": [
    "# %%\n",
    "# Load model and data\n",
    "model = keras.models.load_model(str(MODELS_DIR / 'best_model.h5'))\n",
    "\n",
    "test_df = pd.read_csv(PROCESSED_DIR / 'test_metadata.csv')\n",
    "\n",
    "with open(PROCESSED_DIR / 'data_splits.json', 'r') as f:\n",
    "    split_info = json.load(f)\n",
    "\n",
    "classes = split_info['classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d588cb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and recompiled successfully!\n",
      "❌ test_metadata.csv not found, trying filtered version...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\data\\\\processed\\\\test_metadata_filtered.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     test_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROCESSED_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest_metadata.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Test data loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '..\\\\data\\\\processed\\\\test_metadata.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m❌ test_metadata.csv not found, trying filtered version...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     test_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROCESSED_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest_metadata_filtered.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Filtered test data loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Load class information\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '..\\\\data\\\\processed\\\\test_metadata_filtered.csv'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load model and data dengan error handling\n",
    "try:\n",
    "    # Load model dengan compile=False untuk menghindari warning\n",
    "    model = keras.models.load_model(\n",
    "        str(MODELS_DIR / 'best_model.h5'), \n",
    "        compile=False  # TAMBAH INI\n",
    "    )\n",
    "    \n",
    "    # Recompile model setelah load\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Model loaded and recompiled successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Trying alternative loading method...\")\n",
    "    \n",
    "    # Alternative loading method\n",
    "    try:\n",
    "        model = keras.models.load_model(str(MODELS_DIR / 'best_model.h5'))\n",
    "        print(\"✅ Model loaded with default method!\")\n",
    "    except:\n",
    "        print(\"❌ Still failed. Checking available models...\")\n",
    "        \n",
    "        # List semua model yang available\n",
    "        model_files = list(MODELS_DIR.glob('*.h5'))\n",
    "        print(\"Available model files:\")\n",
    "        for model_file in model_files:\n",
    "            print(f\"  - {model_file.name}\")\n",
    "        \n",
    "        # Load model lain jika best_model tidak ada\n",
    "        if model_files:\n",
    "            model = keras.models.load_model(str(model_files[0]))\n",
    "            print(f\"✅ Loaded alternative model: {model_files[0].name}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No model files found!\")\n",
    "\n",
    "# Load test data\n",
    "try:\n",
    "    test_df = pd.read_csv(PROCESSED_DIR / 'test_metadata.csv')\n",
    "    print(f\"✅ Test data loaded: {len(test_df)} samples\")\n",
    "except:\n",
    "    print(\"❌ test_metadata.csv not found, trying filtered version...\")\n",
    "    test_df = pd.read_csv(PROCESSED_DIR / 'test_metadata_filtered.csv')\n",
    "    print(f\"✅ Filtered test data loaded: {len(test_df)} samples\")\n",
    "\n",
    "# Load class information\n",
    "try:\n",
    "    with open(PROCESSED_DIR / 'data_splits.json', 'r') as f:\n",
    "        split_info = json.load(f)\n",
    "    classes = split_info['classes']\n",
    "    print(f\"✅ Classes loaded: {classes}\")\n",
    "except:\n",
    "    print(\"❌ data_splits.json not found, extracting from dataframe...\")\n",
    "    classes = sorted(test_df['class'].unique().tolist())\n",
    "    print(f\"✅ Classes extracted from dataframe: {classes}\")\n",
    "\n",
    "# Verifikasi model dan data compatibility\n",
    "print(f\"\\n📊 MODEL-DATA COMPATIBILITY CHECK:\")\n",
    "print(f\"Model input shape: {model.input_shape}\")\n",
    "print(f\"Model output shape: {model.output_shape}\")\n",
    "print(f\"Number of classes in data: {len(classes)}\")\n",
    "print(f\"Model output classes: {model.output_shape[1]}\")\n",
    "\n",
    "if model.output_shape[1] != len(classes):\n",
    "    print(\"⚠️  WARNING: Model output dimension doesn't match number of classes!\")\n",
    "    print(\"This might cause issues during evaluation.\")\n",
    "else:\n",
    "    print(\"✅ Model and data classes are compatible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c4a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 9 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 120 invalid image filename(s) in x_col=\"file_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Create test data generator\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='file_path',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    classes=classes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b1969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE MODEL EVALUATION ===\n",
      "🔍 Filtering test data for corrupt images...\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_86.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_75.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_58.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_20.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_30.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_23.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_31.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_0.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_62.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_25.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_60.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_33.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_7.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_41.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_92.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_36.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_57.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_96.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_3.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_41.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_81.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_83.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_33.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_42.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_83.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_92.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_25.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_50.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_23.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_86.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_16.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_78.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_75.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_61.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_99.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_5.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_1.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_12.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_68.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_86.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_11.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_3.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_97.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_64.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_61.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_97.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_74.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_41.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_72.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_37.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_32.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_60.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_3.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_61.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_21.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_24.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_94.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_52.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_10.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_58.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_46.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_32.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_12.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_73.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_49.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_17.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_45.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_67.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_61.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_48.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_54.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_68.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_10.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_7.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_64.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_22.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_2.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_35.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_0.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_82.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_42.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_55.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_63.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_69.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_14.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_48.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_6.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_34.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_93.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_28.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_35.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_68.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_83.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_61.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_69.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_67.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_73.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_19.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_15.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_7.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_95.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_75.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_79.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_23.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\overripe\\overripe_banana_high_quality_photo_89.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\unripe\\unripe_tomato_high_quality_photo_13.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_66.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_15.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_77.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_26.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_78.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\unripe\\unripe_banana_high_quality_photo_83.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\banana\\ripe\\ripe_banana_high_quality_photo_51.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_27.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_3.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\unripe\\unripe_mango_high_quality_photo_38.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\ripe\\ripe_mango_high_quality_photo_67.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\mango\\overripe\\overripe_mango_high_quality_photo_72.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\overripe\\overripe_tomato_high_quality_photo_6.jpg\n",
      "Removing corrupt test image: ..\\data\\raw\\tomato\\ripe\\ripe_tomato_high_quality_photo_27.jpg\n",
      "✅ Valid test images: 0/120\n",
      "📋 Checking DataFrame columns...\n",
      "Available columns: ['file_path', 'fruit', 'ripeness', 'class', 'class_idx']\n",
      "📊 Creating generator with class column: 'class'\n",
      "❌ Evaluation failed: 'class'\n",
      "Trying alternative evaluation method...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Comprehensive evaluation dengan error handling yang diperbaiki\n",
    "print(\"=== COMPREHENSIVE MODEL EVALUATION ===\")\n",
    "\n",
    "def safe_evaluate_model(model, test_df, classes, batch_size=16):\n",
    "    \"\"\"\n",
    "    Evaluate model dengan safety mechanism - FIXED VERSION\n",
    "    \"\"\"\n",
    "    print(\"🔍 Filtering test data for corrupt images...\")\n",
    "    \n",
    "    # Filter test data untuk hanya include valid images\n",
    "    valid_test_indices = []\n",
    "    for idx, row in test_df.iterrows():\n",
    "        try:\n",
    "            with Image.open(row['file_path']) as img:\n",
    "                img.verify()\n",
    "            if os.path.getsize(row['file_path']) > 0:\n",
    "                valid_test_indices.append(idx)\n",
    "        except:\n",
    "            print(f\"Removing corrupt test image: {row['file_path']}\")\n",
    "            continue\n",
    "    \n",
    "    valid_test_df = test_df.loc[valid_test_indices]\n",
    "    print(f\"✅ Valid test images: {len(valid_test_df)}/{len(test_df)}\")\n",
    "    \n",
    "    # CHECK: Pastikan kolom 'class' ada di DataFrame\n",
    "    print(\"📋 Checking DataFrame columns...\")\n",
    "    print(f\"Available columns: {valid_test_df.columns.tolist()}\")\n",
    "    \n",
    "    # Jika kolom 'class' tidak ada, coba kolom alternatif\n",
    "    if 'class' not in valid_test_df.columns:\n",
    "        print(\"⚠️  Column 'class' not found. Looking for alternative columns...\")\n",
    "        \n",
    "        # Coba kolom alternatif yang mungkin ada\n",
    "        possible_class_columns = ['class', 'label', 'category', 'ripeness_class']\n",
    "        class_column_found = None\n",
    "        \n",
    "        for col in possible_class_columns:\n",
    "            if col in valid_test_df.columns:\n",
    "                class_column_found = col\n",
    "                print(f\"✅ Found alternative class column: '{col}'\")\n",
    "                break\n",
    "        \n",
    "        if class_column_found is None:\n",
    "            # Jika tidak ada kolom class, buat dari kombinasi fruit dan ripeness\n",
    "            print(\"🔄 Creating 'class' column from fruit and ripeness...\")\n",
    "            if 'fruit' in valid_test_df.columns and 'ripeness' in valid_test_df.columns:\n",
    "                valid_test_df['class'] = valid_test_df['fruit'] + '_' + valid_test_df['ripeness']\n",
    "                class_column_found = 'class'\n",
    "                print(\"✅ Created 'class' column successfully\")\n",
    "            else:\n",
    "                raise KeyError(\"No class column found and cannot create one from available columns\")\n",
    "    else:\n",
    "        class_column_found = 'class'\n",
    "    \n",
    "    # Create safe test generator dengan kolom yang benar\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    print(f\"📊 Creating generator with class column: '{class_column_found}'\")\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        valid_test_df,\n",
    "        x_col='file_path',\n",
    "        y_col=class_column_found,  # Gunakan kolom yang ditemukan\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False,\n",
    "        classes=classes\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"📊 Evaluating model...\")\n",
    "    if len(valid_test_df) > 0:\n",
    "        test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
    "    else:\n",
    "        print(\"❌ No valid test images available for evaluation!\")\n",
    "        test_loss, test_accuracy = 0, 0\n",
    "    \n",
    "    return test_loss, test_accuracy, valid_test_df\n",
    "\n",
    "# Jalankan safe evaluation\n",
    "try:\n",
    "    test_loss, test_accuracy, valid_test_df = safe_evaluate_model(model, test_df, classes)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Evaluation failed: {e}\")\n",
    "    print(\"Trying alternative evaluation method...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The PyDataset has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[32m      3\u001b[39m y_true = test_generator.classes\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_pred_proba = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m y_pred = np.argmax(y_pred_proba, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Projek AI\\ripeness\\ripenessvision\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:311\u001b[39m, in \u001b[36mPyDatasetAdapter.get_tf_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    306\u001b[39m     batches = [\n\u001b[32m    307\u001b[39m         \u001b[38;5;28mself\u001b[39m._standardize_batch(\u001b[38;5;28mself\u001b[39m.py_dataset[i])\n\u001b[32m    308\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)\n\u001b[32m    309\u001b[39m     ]\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe PyDataset has length 0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m._output_signature = data_adapter_utils.get_tensor_spec(batches)\n\u001b[32m    314\u001b[39m ds = tf.data.Dataset.from_generator(\n\u001b[32m    315\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_iterator,\n\u001b[32m    316\u001b[39m     output_signature=\u001b[38;5;28mself\u001b[39m._output_signature,\n\u001b[32m    317\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: The PyDataset has length 0"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Predictions\n",
    "y_true = test_generator.classes\n",
    "y_pred_proba = model.predict(test_generator)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Detailed classification report\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Enhanced confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classes, yticklabels=classes,\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=classes, yticklabels=classes,\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('Normalized Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00109a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ROC Curve for multiclass\n",
    "def plot_multiclass_roc(y_true, y_pred_proba, classes):\n",
    "    # Binarize the output\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(classes)))\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['blue', 'red', 'green', 'yellow', 'purple', 'orange', 'pink', 'brown', 'gray'])\n",
    "    \n",
    "    for i, color in zip(range(len(classes)), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                ''.format(classes[i], roc_auc[i]))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multiclass ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Plot ROC curve\n",
    "roc_auc = plot_multiclass_roc(y_true, y_pred_proba, classes)\n",
    "print(\"AUC Scores for each class:\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(f\"  {class_name}: {roc_auc[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b10dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Per-class accuracy\n",
    "class_accuracy = {}\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_mask = y_true == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = np.mean(y_pred[class_mask] == y_true[class_mask])\n",
    "        class_accuracy[class_name] = class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75410a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Plot per-class accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(class_accuracy)), list(class_accuracy.values()), \n",
    "               color='skyblue', edgecolor='black')\n",
    "plt.xticks(range(len(class_accuracy)), list(class_accuracy.keys()), rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Per-Class Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, class_accuracy.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Error analysis: misclassified samples\n",
    "misclassified_mask = y_pred != y_true\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "\n",
    "print(f\"Total misclassified samples: {len(misclassified_indices)}\")\n",
    "print(f\"Error rate: {len(misclassified_indices)/len(y_true):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Analyze misclassifications\n",
    "misclassification_analysis = []\n",
    "for idx in misclassified_indices[:20]:  # Show first 20\n",
    "    true_class = classes[y_true[idx]]\n",
    "    pred_class = classes[y_pred[idx]]\n",
    "    confidence = np.max(y_pred_proba[idx])\n",
    "    \n",
    "    misclassification_analysis.append({\n",
    "        'true_class': true_class,\n",
    "        'pred_class': pred_class,\n",
    "        'confidence': confidence,\n",
    "        'file_path': test_df.iloc[idx]['file_path']\n",
    "    })\n",
    "\n",
    "misclassification_df = pd.DataFrame(misclassification_analysis)\n",
    "print(\"Sample misclassifications:\")\n",
    "print(misclassification_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a945bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Visualize misclassified samples\n",
    "def visualize_misclassifications(misclassification_df, num_samples=8):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(num_samples, len(misclassification_df))):\n",
    "        sample = misclassification_df.iloc[i]\n",
    "        img = keras.preprocessing.image.load_img(sample['file_path'])\n",
    "        img_array = keras.preprocessing.image.img_to_array(img)\n",
    "        \n",
    "        axes[i].imshow(img_array / 255.0)\n",
    "        axes[i].set_title(f\"True: {sample['true_class']}\\nPred: {sample['pred_class']}\\nConf: {sample['confidence']:.3f}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_misclassifications(misclassification_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Confidence analysis\n",
    "correct_confidences = []\n",
    "incorrect_confidences = []\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    confidence = np.max(y_pred_proba[i])\n",
    "    if y_pred[i] == y_true[i]:\n",
    "        correct_confidences.append(confidence)\n",
    "    else:\n",
    "        incorrect_confidences.append(confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792dd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Plot confidence distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(correct_confidences, bins=30, alpha=0.7, label='Correct', color='green')\n",
    "plt.hist(incorrect_confidences, bins=30, alpha=0.7, label='Incorrect', color='red')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Confidence Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average confidence - Correct: {np.mean(correct_confidences):.4f}\")\n",
    "print(f\"Average confidence - Incorrect: {np.mean(incorrect_confidences):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b310aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save evaluation results\n",
    "evaluation_results = {\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_loss': float(test_loss),\n",
    "    'per_class_accuracy': class_accuracy,\n",
    "    'auc_scores': {classes[i]: float(roc_auc[i]) for i in range(len(classes))},\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'misclassification_count': int(len(misclassified_indices)),\n",
    "    'error_rate': float(len(misclassified_indices)/len(y_true))\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'evaluation_results.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "# Save misclassification analysis\n",
    "misclassification_df.to_csv(RESULTS_DIR / 'misclassification_analysis.csv', index=False)\n",
    "\n",
    "print(\"Evaluation completed! Results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
